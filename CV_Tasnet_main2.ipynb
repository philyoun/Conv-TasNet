{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "import typing\n",
    "import glob\n",
    "import time\n",
    "import datetime\n",
    "import numpy as np\n",
    "%config Completer.use_jedi = False\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "# Unknownerror, cudnn 어쩌고저쩌고 에러\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import soundfile as sf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# You need to change BATCH_SIZE both in CV_Tasnet_CustomTraining, CV_Tasnet_Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from CV_Tasnet_CustomTraining.ipynb\n"
     ]
    }
   ],
   "source": [
    "import import_ipynb\n",
    "from CV_Tasnet_CustomTraining import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from CV_Tasnet_Model.ipynb\n",
      "tf.__version__: 2.5.0\n",
      "Sample encoder outputs shape: (4, 1999, 512)\n",
      "Before blocks shape: (4, 1999, 128)\n",
      "Total block numbers: 24\n",
      "Number 0 block done\tNumber 1 block done\tNumber 2 block done\tNumber 3 block done\tNumber 4 block done\tNumber 5 block done\tNumber 6 block done\tNumber 7 block done\tNumber 8 block done\tNumber 9 block done\tNumber 10 block done\tNumber 11 block done\tNumber 12 block done\tNumber 13 block done\tNumber 14 block done\tNumber 15 block done\tNumber 16 block done\tNumber 17 block done\tNumber 18 block done\tNumber 19 block done\tNumber 20 block done\tNumber 21 block done\tNumber 22 block done\tNumber 23 block done\t\n",
      "After blocks outputs shape: (4, 1999, 128)\n",
      "(4, 1999, 1024)\n",
      "masks.shape: (4, 2, 1999, 512)\n",
      "Separation_outputs.shape: (4, 2, 1999, 512)\n",
      "Final outputs shape: (4, 16000, 2)\n",
      "Model: \"custom_model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(4, 16000, 1)]      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (4, 1999, 512)       8192        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization (LayerNorma (4, 1999, 512)       1024        conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "bottleneck_layer (Conv1D)       (4, 1999, 128)       65536       layer_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv_block (ConvBlock)          ((4, 1999, 128), (4, 201216      bottleneck_layer[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add (TFOpLambd (4, 1999, 128)       0           conv_block[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv_block_1 (ConvBlock)        ((4, 1999, 128), (4, 201216      conv_block[0][1]                 \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_1 (TFOpLam (4, 1999, 128)       0           tf.__operators__.add[0][0]       \n",
      "                                                                 conv_block_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv_block_2 (ConvBlock)        ((4, 1999, 128), (4, 201216      conv_block_1[0][1]               \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_2 (TFOpLam (4, 1999, 128)       0           tf.__operators__.add_1[0][0]     \n",
      "                                                                 conv_block_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv_block_3 (ConvBlock)        ((4, 1999, 128), (4, 201216      conv_block_2[0][1]               \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_3 (TFOpLam (4, 1999, 128)       0           tf.__operators__.add_2[0][0]     \n",
      "                                                                 conv_block_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv_block_4 (ConvBlock)        ((4, 1999, 128), (4, 201216      conv_block_3[0][1]               \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_4 (TFOpLam (4, 1999, 128)       0           tf.__operators__.add_3[0][0]     \n",
      "                                                                 conv_block_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv_block_5 (ConvBlock)        ((4, 1999, 128), (4, 201216      conv_block_4[0][1]               \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_5 (TFOpLam (4, 1999, 128)       0           tf.__operators__.add_4[0][0]     \n",
      "                                                                 conv_block_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv_block_6 (ConvBlock)        ((4, 1999, 128), (4, 201216      conv_block_5[0][1]               \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_6 (TFOpLam (4, 1999, 128)       0           tf.__operators__.add_5[0][0]     \n",
      "                                                                 conv_block_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv_block_7 (ConvBlock)        ((4, 1999, 128), (4, 201216      conv_block_6[0][1]               \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_7 (TFOpLam (4, 1999, 128)       0           tf.__operators__.add_6[0][0]     \n",
      "                                                                 conv_block_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv_block_8 (ConvBlock)        ((4, 1999, 128), (4, 201216      conv_block_7[0][1]               \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_8 (TFOpLam (4, 1999, 128)       0           tf.__operators__.add_7[0][0]     \n",
      "                                                                 conv_block_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv_block_9 (ConvBlock)        ((4, 1999, 128), (4, 201216      conv_block_8[0][1]               \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_9 (TFOpLam (4, 1999, 128)       0           tf.__operators__.add_8[0][0]     \n",
      "                                                                 conv_block_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv_block_10 (ConvBlock)       ((4, 1999, 128), (4, 201216      conv_block_9[0][1]               \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_10 (TFOpLa (4, 1999, 128)       0           tf.__operators__.add_9[0][0]     \n",
      "                                                                 conv_block_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv_block_11 (ConvBlock)       ((4, 1999, 128), (4, 201216      conv_block_10[0][1]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_11 (TFOpLa (4, 1999, 128)       0           tf.__operators__.add_10[0][0]    \n",
      "                                                                 conv_block_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv_block_12 (ConvBlock)       ((4, 1999, 128), (4, 201216      conv_block_11[0][1]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_12 (TFOpLa (4, 1999, 128)       0           tf.__operators__.add_11[0][0]    \n",
      "                                                                 conv_block_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv_block_13 (ConvBlock)       ((4, 1999, 128), (4, 201216      conv_block_12[0][1]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_13 (TFOpLa (4, 1999, 128)       0           tf.__operators__.add_12[0][0]    \n",
      "                                                                 conv_block_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv_block_14 (ConvBlock)       ((4, 1999, 128), (4, 201216      conv_block_13[0][1]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_14 (TFOpLa (4, 1999, 128)       0           tf.__operators__.add_13[0][0]    \n",
      "                                                                 conv_block_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv_block_15 (ConvBlock)       ((4, 1999, 128), (4, 201216      conv_block_14[0][1]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_15 (TFOpLa (4, 1999, 128)       0           tf.__operators__.add_14[0][0]    \n",
      "                                                                 conv_block_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv_block_16 (ConvBlock)       ((4, 1999, 128), (4, 201216      conv_block_15[0][1]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_16 (TFOpLa (4, 1999, 128)       0           tf.__operators__.add_15[0][0]    \n",
      "                                                                 conv_block_16[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv_block_17 (ConvBlock)       ((4, 1999, 128), (4, 201216      conv_block_16[0][1]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_17 (TFOpLa (4, 1999, 128)       0           tf.__operators__.add_16[0][0]    \n",
      "                                                                 conv_block_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv_block_18 (ConvBlock)       ((4, 1999, 128), (4, 201216      conv_block_17[0][1]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_18 (TFOpLa (4, 1999, 128)       0           tf.__operators__.add_17[0][0]    \n",
      "                                                                 conv_block_18[0][0]              \n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv_block_19 (ConvBlock)       ((4, 1999, 128), (4, 201216      conv_block_18[0][1]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_19 (TFOpLa (4, 1999, 128)       0           tf.__operators__.add_18[0][0]    \n",
      "                                                                 conv_block_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv_block_20 (ConvBlock)       ((4, 1999, 128), (4, 201216      conv_block_19[0][1]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_20 (TFOpLa (4, 1999, 128)       0           tf.__operators__.add_19[0][0]    \n",
      "                                                                 conv_block_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv_block_21 (ConvBlock)       ((4, 1999, 128), (4, 201216      conv_block_20[0][1]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_21 (TFOpLa (4, 1999, 128)       0           tf.__operators__.add_20[0][0]    \n",
      "                                                                 conv_block_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv_block_22 (ConvBlock)       ((4, 1999, 128), (4, 201216      conv_block_21[0][1]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_22 (TFOpLa (4, 1999, 128)       0           tf.__operators__.add_21[0][0]    \n",
      "                                                                 conv_block_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv_block_23 (ConvBlock)       ((4, 1999, 128), (4, 201216      conv_block_22[0][1]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_23 (TFOpLa (4, 1999, 128)       0           tf.__operators__.add_22[0][0]    \n",
      "                                                                 conv_block_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu (PReLU)                 (4, 1999, 128)       128         tf.__operators__.add_23[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "bottleneck_layer2 (Conv1D)      (4, 1999, 1024)      131072      p_re_lu[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf.reshape (TFOpLambda)         (4, 2, 1999, 512)    0           bottleneck_layer2[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.softmax (TFOpLa (4, 2, 1999, 512)    0           tf.reshape[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "multiply (Multiply)             (4, 2, 1999, 512)    0           conv1d[0][0]                     \n",
      "                                                                 tf.compat.v1.nn.softmax[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.getitem (Slici (4, 1999, 512)       0           multiply[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.getitem_1 (Sli (4, 1999, 512)       0           multiply[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "Decoder_Num0 (Conv1DTranspose)  (4, 16000, 1)        8192        tf.__operators__.getitem[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "Decoder_Num1 (Conv1DTranspose)  (4, 16000, 1)        8192        tf.__operators__.getitem_1[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "tf.concat (TFOpLambda)          (4, 16000, 2)        0           Decoder_Num0[0][0]               \n",
      "                                                                 Decoder_Num1[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 5,051,520\n",
      "Trainable params: 5,051,520\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import import_ipynb\n",
    "from CV_Tasnet_Model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_base_dir = \"C:/Users/Phil/anaconda3/envs/LibriMix/storage_dir/Libri2Mix/wav8k/min\"\n",
    "\n",
    "train_input_path = 'train-360/mix_both'\n",
    "train_target_paths = ['train-360/s1', 'train-360/s2']\n",
    "\n",
    "val_input_path = 'dev/mix_both'\n",
    "val_target_paths = ['dev/s1', 'dev/s2']\n",
    "\n",
    "test_input_path = 'test/mix_both'\n",
    "test_target_paths = ['test/s1', 'test/s2']\n",
    "\n",
    "ext = '/*.wav'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store a path\n",
    "train_input_dir = os.path.join(data_base_dir, train_input_path)\n",
    "train_target_dirs = [os.path.join(data_base_dir, train_path) \n",
    "                     for train_path in train_target_paths]\n",
    "\n",
    "val_input_dir = os.path.join(data_base_dir, val_input_path)\n",
    "val_target_dirs =[os.path.join(data_base_dir, val_path) \n",
    "                  for val_path in val_target_paths]\n",
    "\n",
    "test_input_dir = os.path.join(data_base_dir, test_input_path)\n",
    "test_target_dirs = [os.path.join(data_base_dir, test_path)\n",
    "                   for test_path in test_target_paths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find every files in a selected path\n",
    "train_input_files = glob.glob(train_input_dir + ext)\n",
    "train_target_files = [glob.glob(train_path + ext) for train_path in train_target_dirs]\n",
    "\n",
    "val_input_files = glob.glob(val_input_dir + ext)\n",
    "val_target_files = [glob.glob(val_path + ext) for val_path in val_target_dirs]\n",
    "\n",
    "test_input_files = glob.glob(test_input_dir + ext)\n",
    "test_target_files = [glob.glob(test_path + ext) for test_path in test_target_dirs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training files: 50800\n",
      "Number of validation files: 3000\n",
      "Number of test files: 3000\n"
     ]
    }
   ],
   "source": [
    "print('Number of training files:', len(train_input_files))\n",
    "for i in [len(j) for j in train_target_files]:\n",
    "    assert len(train_input_files) == i\n",
    "\n",
    "print('Number of validation files:', len(val_input_files))\n",
    "for i in [len(j) for j in val_target_files]:\n",
    "    assert len(val_input_files) == i\n",
    "\n",
    "print('Number of test files:', len(test_input_files))\n",
    "for i in [len(j) for j in test_target_files]:\n",
    "    assert len(test_input_files) == i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.getLogger('tensorflow').setLevel(logging.ERROR)\n",
    "\n",
    "# WARNING:tensorflow:Gradients do not exist for variables ['conv_block_23/res_conv/kernel:0'] when minimizing the loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:/Users/Phil/anaconda3/envs/CTnet\\\\checkpoints/cp-{epoch:03d}.ckpt'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_base_dir = \"C:/Users/Phil/anaconda3/envs/CTnet\"\n",
    "checkpoint_path = \"checkpoints/cp-{epoch:03d}.ckpt\"\n",
    "saving_path = os.path.join(model_base_dir, checkpoint_path)\n",
    "saving_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_callback = keras.callbacks.ModelCheckpoint(filepath=saving_path,\n",
    "                                                 verbose=1,\n",
    "                                                 save_weights_only=True,\n",
    "                                                 save_freq='epoch')\n",
    "callbacks = keras.callbacks.CallbackList([model_callback], model=cv_tasnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:/Users/Phil/anaconda3/envs/CTnet\\\\checkpoints/cp-{epoch:03d}.ckpt'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saving_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
    "# log_path = os.path.join(model_base_dir, \"logs\", now)\n",
    "# summary_writer = tf.summary.create_file_writer(log_path)\n",
    "# log_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 여기선 model.fit()을 이용하지 않고서 custom training을 할거기 때문에 compile은 패스\n",
    "# cv_tasnet.compile(optimizer=keras.optimizers.Adam(0.001, clipnorm=5.),\n",
    "#                  loss=SI_SNRloss(),\n",
    "#                  metrics=[SI_SNRmetric()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = keras.optimizers.Adam(0.001, clipnorm=5.)\n",
    "loss_fn = SI_SNRloss(name='si_snr')\n",
    "val_loss_fn = SI_SNRloss(name='val_si_snr')\n",
    "metrics = SI_SNRmetric()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# @tf.function\n",
    "# def train_step(input_batch, target_batch, epoch, step, i):\n",
    "#     with tf.GradientTape() as tape:\n",
    "#         predicted_batch = cv_tasnet(input_batch, training=True)\n",
    "#         loss = loss_fn(target_batch, predicted_batch)\n",
    "#     trainable_vars = cv_tasnet.trainable_variables\n",
    "#     gradients = tape.gradient(loss, trainable_vars)\n",
    "#     optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "    \n",
    "#     with summary_writer.as_default():\n",
    "#         tf.summary.scalar('train_sisnr', loss, step=i*(step+1)*(epoch+1))\n",
    "#     return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(input_batch, target_batch):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predicted_batch = cv_tasnet(input_batch, training=True)\n",
    "        loss = loss_fn(target_batch, predicted_batch)\n",
    "    trainable_vars = cv_tasnet.trainable_variables\n",
    "    gradients = tape.gradient(loss, trainable_vars)\n",
    "    optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def test_step(val_input_batch, val_target_batch):\n",
    "    val_predicted_batch = cv_tasnet(val_input_batch, training=False)\n",
    "    val_loss = val_loss_fn(val_target_batch, val_predicted_batch)\n",
    "    return val_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# now = datetime.datetime.now().strftime(\"%m%d%H%M%S\")\n",
    "# logs = \"C:/Users/Phil/ml/logs/\" + now\n",
    "# tboard_callback1 = keras.callbacks.TensorBoard(log_dir=logs,\n",
    "#                                               histogram_freq=1,\n",
    "#                                               profile_batch='500, 520')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_gen_callable_mini(files1, files2, files3, per_step=per_step, seconds=seconds, batch_size=BATCH_SIZE):\n",
    "    def gen_blocks(files1=files1, files2=files2, files3=files3):\n",
    "        train_blocks = []\n",
    "        target_blocks1 = []\n",
    "        target_blocks2 = []\n",
    "        \n",
    "        for j in range(per_step):\n",
    "            train_blocks += [block for block in sf.blocks(files1[j], blocksize=seconds)][:-1]\n",
    "            target_blocks1 += [block for block in sf.blocks(files2[j], blocksize=seconds)][:-1]\n",
    "            target_blocks2 += [block for block in sf.blocks(files3[j], blocksize=seconds)][:-1]\n",
    "        for k in range(len(train_blocks) // batch_size):\n",
    "            train_batch = train_blocks[k*batch_size:(k+1)*batch_size]\n",
    "            target_batch1 = target_blocks1[k*batch_size:(k+1)*batch_size]\n",
    "            target_batch2 = target_blocks2[k*batch_size:(k+1)*batch_size]\n",
    "\n",
    "            train_batch = tf.expand_dims(tf.convert_to_tensor(train_batch), axis=-1)\n",
    "            target_batch1 = tf.expand_dims(tf.convert_to_tensor(target_batch1), axis=-1)\n",
    "            target_batch2 = tf.expand_dims(tf.convert_to_tensor(target_batch2), axis=-1)\n",
    "            yield train_batch, tf.concat([target_batch1, target_batch2], axis=-1)\n",
    "    return gen_blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################### Start of Epoch Num 1, out of 3 Epochs ###################\n",
      "========================== Training part Begins ==========================\n",
      "1/16, Current iteration: 3771, SI_SNR: -8.43751\n",
      "2/16, Current iteration: 3786, SI_SNR: -9.13742\n",
      "3/16, Current iteration: 3733, SI_SNR: -9.43301\n",
      "4/16, Current iteration: 3832, SI_SNR: -8.10961\n",
      "5/16, Current iteration: 3783, SI_SNR: -5.73900\n",
      "6/16, Current iteration: 3758, SI_SNR: -8.83705\n",
      "7/16, Current iteration: 3782, SI_SNR: -8.41752\n",
      "8/16, Current iteration: 3763, SI_SNR: -9.14479\n",
      "9/16, Current iteration: 3792, SI_SNR: -10.6101\n",
      "10/16, Current iteration: 3796, SI_SNR: -5.57596\n",
      "11/16, Current iteration: 3832, SI_SNR: -9.08526\n",
      "12/16, Current iteration: 3712, SI_SNR: -11.6656\n",
      "13/16, Current iteration: 3790, SI_SNR: -11.2115\n",
      "14/16, Current iteration: 3758, SI_SNR: -6.88985\n",
      "15/16, Current iteration: 3782, SI_SNR: -7.36020\n",
      "16/16, Current iteration: 3800, SI_SNR: -10.7554\n",
      "============= Training part Done, time taken: 442326.28 seconds =============\n",
      "\n",
      "========================== Validation part Begins ==========================\n",
      "1/1, Val Current iteration: 1640, val_SI_SNR: -6.96604\n",
      "============= Validation part Done, time taken: 2982.17 seconds =============\n",
      "\n",
      "Epoch 00001: saving model to C:/Users/Phil/anaconda3/envs/CTnet\\checkpoints\\cp-001.ckpt\n",
      "############### End of Epoch Num 1, time taken: 445312.92 seconds ###############\n",
      "\n",
      "\n",
      "################### Start of Epoch Num 2, out of 3 Epochs ###################\n",
      "========================== Training part Begins ==========================\n",
      "1/16, Current iteration: 3797, SI_SNR: -9.91572\n",
      "2/16, Current iteration: 3781, SI_SNR: -5.70473\n",
      "3/16, Current iteration: 3785, SI_SNR: -7.53627\n",
      "4/16, Current iteration: 3767, SI_SNR: -14.7699\n",
      "5/16, Current iteration: 3789, SI_SNR: -7.12462\n",
      "6/16, Current iteration: 3809, SI_SNR: -9.48869\n",
      "7/16, Current iteration: 3775, SI_SNR: -12.3234\n",
      "8/16, Current iteration: 3746, SI_SNR: -9.11568\n",
      "9/16, Current iteration: 3732, SI_SNR: -10.1415\n",
      "10/16, Current iteration: 3735, SI_SNR: -8.61195\n",
      "11/16, Current iteration: 3784, SI_SNR: -8.94277\n",
      "12/16, Current iteration: 2948, SI_SNR: -11.1526"
     ]
    }
   ],
   "source": [
    "EPOCHS = 3\n",
    "per_step = 3000 # 램과 상관이 있음\n",
    "N_steps = len(train_input_files) // per_step\n",
    "val_N_steps = len(val_input_files) // per_step\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print('################### Start of Epoch Num {}, out of {} Epochs ###################'.format(epoch+1, \n",
    "                                                                                           EPOCHS))\n",
    "    epoch_time = time.time()\n",
    "    idx_lst = np.arange(len(train_input_files))\n",
    "    np.random.shuffle(idx_lst)\n",
    "\n",
    "    train_time = time.time()\n",
    "    print('========================== Training part Begins ==========================')\n",
    "    for step in range(N_steps):\n",
    "        train_selected = [train_input_files[idx] for idx in idx_lst[step*per_step:(step+1)*per_step]]\n",
    "        target_selected1 = [train_target_files[0][idx] for idx in idx_lst[step*per_step:(step+1)*per_step]]\n",
    "        target_selected2 = [train_target_files[1][idx] for idx in idx_lst[step*per_step:(step+1)*per_step]]\n",
    "        \n",
    "        train_gen_block = make_gen_callable_mini(train_selected, target_selected1, target_selected2, per_step=per_step, \n",
    "                                          seconds=seconds, batch_size=BATCH_SIZE)\n",
    "        train_ds = tf.data.Dataset.from_generator(train_gen_block, output_signature=(\n",
    "            tf.TensorSpec(shape=(BATCH_SIZE, seconds, 1), dtype=tf.float64), \n",
    "            tf.TensorSpec(shape=(BATCH_SIZE, seconds, 2), dtype=tf.float64)))\n",
    "        train_ds = train_ds.prefetch(tf.data.AUTOTUNE)\n",
    "        \n",
    "        for i, (input_batch, target_batch) in enumerate(train_ds):\n",
    "            loss = train_step(input_batch, target_batch)\n",
    "#             loss = train_step(input_batch, target_batch, epoch, step, i)\n",
    "            print(\"\\r{}/{}, Current iteration: {}, SI_SNR: {:.4f}\".format(step+1, N_steps, i, loss.numpy()), end='')\n",
    "        print()\n",
    "    print('============= Training part Done, time taken: {:.2f} seconds ============='.format(time.time()-train_time))\n",
    "    print()\n",
    "    \n",
    "    #### validation part\n",
    "    val_idx_lst = np.arange(len(val_input_files))\n",
    "    np.random.shuffle(val_idx_lst)\n",
    "    \n",
    "    val_time = time.time()\n",
    "    print('========================== Validation part Begins ==========================')\n",
    "    for val_step in range(val_N_steps):\n",
    "        val_train_selected = [val_input_files[val_idx] for val_idx in val_idx_lst[val_step*per_step:(val_step+1)*per_step]]\n",
    "        val_target_selected1 = [val_target_files[0][val_idx] for val_idx in val_idx_lst[val_step*per_step:(val_step+1)*per_step]]\n",
    "        val_target_selected2 = [val_target_files[1][val_idx] for val_idx in val_idx_lst[val_step*per_step:(val_step+1)*per_step]]\n",
    "        \n",
    "        val_gen_block = make_gen_callable_mini(val_train_selected, val_target_selected1, val_target_selected2, per_step=per_step,\n",
    "                                              seconds=seconds, batch_size=BATCH_SIZE)\n",
    "        val_ds = tf.data.Dataset.from_generator(val_gen_block, output_signature=(\n",
    "            tf.TensorSpec(shape=(BATCH_SIZE, seconds, 1), dtype=tf.float64),\n",
    "            tf.TensorSpec(shape=(BATCH_SIZE, seconds, 2), dtype=tf.float64)))\n",
    "        val_ds = val_ds.prefetch(tf.data.AUTOTUNE)\n",
    "        \n",
    "        for j, (val_input_batch, val_target_batch) in enumerate(val_ds):\n",
    "            val_loss = test_step(val_input_batch, val_target_batch)\n",
    "            print('\\r{}/{}, Val Current iteration: {}, val_SI_SNR: {:.4f}'.format(val_step+1, val_N_steps, j, val_loss.numpy()), end='')\n",
    "        print()\n",
    "    print('============= Validation part Done, time taken: {:.2f} seconds ============='.format(time.time()-val_time))\n",
    "    \n",
    "    callbacks.on_epoch_end(epoch, logs=None)\n",
    "    \n",
    "    print('############### End of Epoch Num {}, time taken: {:.2f} seconds ###############'.format(epoch + 1, \n",
    "                                                                                    time.time() - epoch_time))\n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:/Users/Phil/anaconda3/envs/CTnet/checkpoints\\\\cp-001.ckpt'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# My computer shorted out because of my cpu cooler is broken..\n",
    "# It takes such a long time to train, so that I decided to continue the training where I left off,\n",
    "# with a little bit of change. (to display median of the training losses(si_snr), and the time taken for iterations.)\n",
    "\n",
    "latest = tf.train.latest_checkpoint(\"C:/Users/Phil/anaconda3/envs/CTnet/checkpoints\")\n",
    "latest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x11f874e5850>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cv_tasnet.load_weights(latest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# median\n",
    "# https://stackoverflow.com/questions/43824665/tensorflow-median-value\n",
    "def get_median(v):\n",
    "    v = tf.reshape(v, [-1])\n",
    "    m = v.get_shape()[0]//2\n",
    "    return tf.reduce_min(tf.nn.top_k(v, m, sorted=False).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################### Start of Epoch Num 1, out of 3 Epochs ###################\n",
      "========================== Training part Begins ==========================\n",
      "1/16, Current iteration: 3797, SI_SNR: -8.7440, median of losses: -7.8221, taken time: 30573.73 seconds\n",
      "2/16, Current iteration: 3773, SI_SNR: -4.2347, median of losses: -8.5831, taken time: 28574.14 seconds\n",
      "3/16, Current iteration: 3827, SI_SNR: -8.2543, median of losses: -8.6908, taken time: 28964.39 seconds\n",
      "4/16, Current iteration: 3740, SI_SNR: -6.0091, median of losses: -8.6637, taken time: 28376.02 seconds\n",
      "5/16, Current iteration: 3797, SI_SNR: -7.0006, median of losses: -8.7839, taken time: 28772.33 seconds\n",
      "6/16, Current iteration: 3736, SI_SNR: -9.2620, median of losses: -8.6714, taken time: 28375.78 seconds\n",
      "7/16, Current iteration: 3785, SI_SNR: -5.3856, median of losses: -8.6852, taken time: 28714.25 seconds\n",
      "8/16, Current iteration: 3783, SI_SNR: -3.2544, median of losses: -8.6821, taken time: 28710.30 seconds\n",
      "9/16, Current iteration: 3762, SI_SNR: -5.4751, median of losses: -8.7014, taken time: 28634.97 seconds\n",
      "10/16, Current iteration: 3812, SI_SNR: -14.9539, median of losses: -8.7524, taken time: 29073.28 seconds\n",
      "11/16, Current iteration: 3762, SI_SNR: -11.8078, median of losses: -8.8570, taken time: 28494.31 seconds\n",
      "12/16, Current iteration: 3740, SI_SNR: -8.8687, median of losses: -8.7464, taken time: 28316.78 seconds\n",
      "13/16, Current iteration: 3789, SI_SNR: -12.2263, median of losses: -8.8146, taken time: 28709.38 seconds\n",
      "14/16, Current iteration: 3538, SI_SNR: -9.7987,  "
     ]
    }
   ],
   "source": [
    "EPOCHS = 3\n",
    "per_step = 3000 # 램과 상관이 있음\n",
    "N_steps = len(train_input_files) // per_step\n",
    "val_N_steps = len(val_input_files) // per_step\n",
    "\n",
    "epoch = 1++++++++++++++++++++++++\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    print('################### Start of Epoch Num {}, out of {} Epochs ###################'.format(epoch+1, \n",
    "                                                                                           EPOCHS))\n",
    "    epoch_time = time.time()\n",
    "    idx_lst = np.arange(len(train_input_files))\n",
    "    np.random.shuffle(idx_lst)\n",
    "\n",
    "    train_time = time.time()\n",
    "    print('========================== Training part Begins ==========================')\n",
    "    for step in range(N_steps):\n",
    "        train_selected = [train_input_files[idx] for idx in idx_lst[step*per_step:(step+1)*per_step]]\n",
    "        target_selected1 = [train_target_files[0][idx] for idx in idx_lst[step*per_step:(step+1)*per_step]]\n",
    "        target_selected2 = [train_target_files[1][idx] for idx in idx_lst[step*per_step:(step+1)*per_step]]\n",
    "        \n",
    "        train_gen_block = make_gen_callable_mini(train_selected, target_selected1, target_selected2, per_step=per_step, \n",
    "                                          seconds=seconds, batch_size=BATCH_SIZE)\n",
    "        train_ds = tf.data.Dataset.from_generator(train_gen_block, output_signature=(\n",
    "            tf.TensorSpec(shape=(BATCH_SIZE, seconds, 1), dtype=tf.float64), \n",
    "            tf.TensorSpec(shape=(BATCH_SIZE, seconds, 2), dtype=tf.float64)))\n",
    "        train_ds = train_ds.prefetch(tf.data.AUTOTUNE)\n",
    "        \n",
    "        losses = []\n",
    "        iter_time = time.time()\n",
    "        for i, (input_batch, target_batch) in enumerate(train_ds):\n",
    "            loss = train_step(input_batch, target_batch)\n",
    "#             loss = train_step(input_batch, target_batch, epoch, step, i)\n",
    "            losses.append(loss.numpy())\n",
    "            print(\"\\r{}/{}, Current iteration: {}, SI_SNR: {:.4f}, \".format(step+1, N_steps, i, loss.numpy()), end='')\n",
    "        print('median of losses: {:.4f}, taken time: {:.2f} seconds'.format(get_median(losses), time.time()-iter_time))\n",
    "    print('============= Training part Done, time taken: {:.2f} seconds ============='.format(time.time()-train_time))\n",
    "    print()\n",
    "    \n",
    "    #### validation part\n",
    "    val_idx_lst = np.arange(len(val_input_files))\n",
    "    np.random.shuffle(val_idx_lst)\n",
    "    \n",
    "    val_time = time.time()\n",
    "    print('========================== Validation part Begins ==========================')\n",
    "    for val_step in range(val_N_steps):\n",
    "        val_train_selected = [val_input_files[val_idx] for val_idx in val_idx_lst[val_step*per_step:(val_step+1)*per_step]]\n",
    "        val_target_selected1 = [val_target_files[0][val_idx] for val_idx in val_idx_lst[val_step*per_step:(val_step+1)*per_step]]\n",
    "        val_target_selected2 = [val_target_files[1][val_idx] for val_idx in val_idx_lst[val_step*per_step:(val_step+1)*per_step]]\n",
    "        \n",
    "        val_gen_block = make_gen_callable_mini(val_train_selected, val_target_selected1, val_target_selected2, per_step=per_step,\n",
    "                                              seconds=seconds, batch_size=BATCH_SIZE)\n",
    "        val_ds = tf.data.Dataset.from_generator(val_gen_block, output_signature=(\n",
    "            tf.TensorSpec(shape=(BATCH_SIZE, seconds, 1), dtype=tf.float64),\n",
    "            tf.TensorSpec(shape=(BATCH_SIZE, seconds, 2), dtype=tf.float64)))\n",
    "        val_ds = val_ds.prefetch(tf.data.AUTOTUNE)\n",
    "        \n",
    "        val_losses = []\n",
    "        iter_time2 = time.time()\n",
    "        for j, (val_input_batch, val_target_batch) in enumerate(val_ds):\n",
    "            val_loss = test_step(val_input_batch, val_target_batch)\n",
    "            print('\\r{}/{}, Val Current iteration: {}, val_SI_SNR: {:.4f}'.format(val_step+1, val_N_steps, j, val_loss.numpy()), end='')\n",
    "        print('median of losses: {:.4f}, taken time: {:.2f} seconds'.format(get_median(val_losses), time.time()-iter_time2))\n",
    "    print('============= Validation part Done, time taken: {:.2f} seconds ============='.format(time.time()-val_time))\n",
    "    \n",
    "    callbacks.on_epoch_end(epoch, logs=None)\n",
    "    \n",
    "    print('############### End of Epoch Num {}, time taken: {:.2f} seconds ###############'.format(epoch + 1, \n",
    "                                                                                    time.time() - epoch_time))\n",
    "    print()\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
