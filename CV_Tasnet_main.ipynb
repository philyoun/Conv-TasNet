{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "import typing\n",
    "import glob\n",
    "import time\n",
    "import datetime\n",
    "import numpy as np\n",
    "%config Completer.use_jedi = False\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "# Unknownerror, cudnn 어쩌고저쩌고 에러\n",
    "gpus = tf.config.experimental.list_physical_devices('GPU')\n",
    "if gpus:\n",
    "    try:\n",
    "        # Currently, memory growth needs to be the same across GPUs\n",
    "        for gpu in gpus:\n",
    "            tf.config.experimental.set_memory_growth(gpu, True)\n",
    "    except RuntimeError as e:\n",
    "        # Memory growth must be set before GPUs have been initialized\n",
    "        print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.5.0\n"
     ]
    }
   ],
   "source": [
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function sys.getsizeof>"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "import random\n",
    "import soundfile as sf\n",
    "sys.getsizeof"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from CV_Tasnet_CustomTraining.ipynb\n"
     ]
    }
   ],
   "source": [
    "import import_ipynb\n",
    "from CV_Tasnet_CustomTraining import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from CV_Tasnet_Model.ipynb\n",
      "tf.__version__: 2.5.0\n",
      "Sample encoder outputs shape: (8, 1999, 512)\n",
      "Before blocks shape: (8, 1999, 128)\n",
      "Total block numbers: 24\n",
      "Number 0 block done\tNumber 1 block done\tNumber 2 block done\tNumber 3 block done\tNumber 4 block done\tNumber 5 block done\tNumber 6 block done\tNumber 7 block done\tNumber 8 block done\tNumber 9 block done\tNumber 10 block done\tNumber 11 block done\tNumber 12 block done\tNumber 13 block done\tNumber 14 block done\tNumber 15 block done\tNumber 16 block done\tNumber 17 block done\tNumber 18 block done\tNumber 19 block done\tNumber 20 block done\tNumber 21 block done\tNumber 22 block done\tNumber 23 block done\t\n",
      "After blocks outputs shape: (8, 1999, 128)\n",
      "(8, 1999, 1024)\n",
      "masks.shape: (8, 2, 1999, 512)\n",
      "Separation_outputs.shape: (8, 2, 1999, 512)\n",
      "Final outputs shape: (8, 16000, 2)\n",
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            [(8, 16000, 1)]      0                                            \n",
      "__________________________________________________________________________________________________\n",
      "conv1d (Conv1D)                 (8, 1999, 512)       8192        input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "layer_normalization (LayerNorma (8, 1999, 512)       1024        conv1d[0][0]                     \n",
      "__________________________________________________________________________________________________\n",
      "bottleneck_layer (Conv1D)       (8, 1999, 128)       65536       layer_normalization[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "conv_block (ConvBlock)          ((8, 1999, 128), (8, 201216      bottleneck_layer[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add (TFOpLambd (8, 1999, 128)       0           conv_block[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "conv_block_1 (ConvBlock)        ((8, 1999, 128), (8, 201216      conv_block[0][1]                 \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_1 (TFOpLam (8, 1999, 128)       0           tf.__operators__.add[0][0]       \n",
      "                                                                 conv_block_1[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv_block_2 (ConvBlock)        ((8, 1999, 128), (8, 201216      conv_block_1[0][1]               \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_2 (TFOpLam (8, 1999, 128)       0           tf.__operators__.add_1[0][0]     \n",
      "                                                                 conv_block_2[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv_block_3 (ConvBlock)        ((8, 1999, 128), (8, 201216      conv_block_2[0][1]               \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_3 (TFOpLam (8, 1999, 128)       0           tf.__operators__.add_2[0][0]     \n",
      "                                                                 conv_block_3[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv_block_4 (ConvBlock)        ((8, 1999, 128), (8, 201216      conv_block_3[0][1]               \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_4 (TFOpLam (8, 1999, 128)       0           tf.__operators__.add_3[0][0]     \n",
      "                                                                 conv_block_4[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv_block_5 (ConvBlock)        ((8, 1999, 128), (8, 201216      conv_block_4[0][1]               \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_5 (TFOpLam (8, 1999, 128)       0           tf.__operators__.add_4[0][0]     \n",
      "                                                                 conv_block_5[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv_block_6 (ConvBlock)        ((8, 1999, 128), (8, 201216      conv_block_5[0][1]               \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_6 (TFOpLam (8, 1999, 128)       0           tf.__operators__.add_5[0][0]     \n",
      "                                                                 conv_block_6[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv_block_7 (ConvBlock)        ((8, 1999, 128), (8, 201216      conv_block_6[0][1]               \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_7 (TFOpLam (8, 1999, 128)       0           tf.__operators__.add_6[0][0]     \n",
      "                                                                 conv_block_7[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv_block_8 (ConvBlock)        ((8, 1999, 128), (8, 201216      conv_block_7[0][1]               \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_8 (TFOpLam (8, 1999, 128)       0           tf.__operators__.add_7[0][0]     \n",
      "                                                                 conv_block_8[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv_block_9 (ConvBlock)        ((8, 1999, 128), (8, 201216      conv_block_8[0][1]               \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_9 (TFOpLam (8, 1999, 128)       0           tf.__operators__.add_8[0][0]     \n",
      "                                                                 conv_block_9[0][0]               \n",
      "__________________________________________________________________________________________________\n",
      "conv_block_10 (ConvBlock)       ((8, 1999, 128), (8, 201216      conv_block_9[0][1]               \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_10 (TFOpLa (8, 1999, 128)       0           tf.__operators__.add_9[0][0]     \n",
      "                                                                 conv_block_10[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv_block_11 (ConvBlock)       ((8, 1999, 128), (8, 201216      conv_block_10[0][1]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_11 (TFOpLa (8, 1999, 128)       0           tf.__operators__.add_10[0][0]    \n",
      "                                                                 conv_block_11[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv_block_12 (ConvBlock)       ((8, 1999, 128), (8, 201216      conv_block_11[0][1]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_12 (TFOpLa (8, 1999, 128)       0           tf.__operators__.add_11[0][0]    \n",
      "                                                                 conv_block_12[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv_block_13 (ConvBlock)       ((8, 1999, 128), (8, 201216      conv_block_12[0][1]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_13 (TFOpLa (8, 1999, 128)       0           tf.__operators__.add_12[0][0]    \n",
      "                                                                 conv_block_13[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv_block_14 (ConvBlock)       ((8, 1999, 128), (8, 201216      conv_block_13[0][1]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_14 (TFOpLa (8, 1999, 128)       0           tf.__operators__.add_13[0][0]    \n",
      "                                                                 conv_block_14[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv_block_15 (ConvBlock)       ((8, 1999, 128), (8, 201216      conv_block_14[0][1]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_15 (TFOpLa (8, 1999, 128)       0           tf.__operators__.add_14[0][0]    \n",
      "                                                                 conv_block_15[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv_block_16 (ConvBlock)       ((8, 1999, 128), (8, 201216      conv_block_15[0][1]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_16 (TFOpLa (8, 1999, 128)       0           tf.__operators__.add_15[0][0]    \n",
      "                                                                 conv_block_16[0][0]              \n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "conv_block_17 (ConvBlock)       ((8, 1999, 128), (8, 201216      conv_block_16[0][1]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_17 (TFOpLa (8, 1999, 128)       0           tf.__operators__.add_16[0][0]    \n",
      "                                                                 conv_block_17[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv_block_18 (ConvBlock)       ((8, 1999, 128), (8, 201216      conv_block_17[0][1]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_18 (TFOpLa (8, 1999, 128)       0           tf.__operators__.add_17[0][0]    \n",
      "                                                                 conv_block_18[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv_block_19 (ConvBlock)       ((8, 1999, 128), (8, 201216      conv_block_18[0][1]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_19 (TFOpLa (8, 1999, 128)       0           tf.__operators__.add_18[0][0]    \n",
      "                                                                 conv_block_19[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv_block_20 (ConvBlock)       ((8, 1999, 128), (8, 201216      conv_block_19[0][1]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_20 (TFOpLa (8, 1999, 128)       0           tf.__operators__.add_19[0][0]    \n",
      "                                                                 conv_block_20[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv_block_21 (ConvBlock)       ((8, 1999, 128), (8, 201216      conv_block_20[0][1]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_21 (TFOpLa (8, 1999, 128)       0           tf.__operators__.add_20[0][0]    \n",
      "                                                                 conv_block_21[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv_block_22 (ConvBlock)       ((8, 1999, 128), (8, 201216      conv_block_21[0][1]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_22 (TFOpLa (8, 1999, 128)       0           tf.__operators__.add_21[0][0]    \n",
      "                                                                 conv_block_22[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "conv_block_23 (ConvBlock)       ((8, 1999, 128), (8, 201216      conv_block_22[0][1]              \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.add_23 (TFOpLa (8, 1999, 128)       0           tf.__operators__.add_22[0][0]    \n",
      "                                                                 conv_block_23[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "p_re_lu (PReLU)                 (8, 1999, 128)       128         tf.__operators__.add_23[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "bottleneck_layer2 (Conv1D)      (8, 1999, 1024)      131072      p_re_lu[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "tf.reshape (TFOpLambda)         (8, 2, 1999, 512)    0           bottleneck_layer2[0][0]          \n",
      "__________________________________________________________________________________________________\n",
      "tf.compat.v1.nn.softmax (TFOpLa (8, 2, 1999, 512)    0           tf.reshape[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "multiply (Multiply)             (8, 2, 1999, 512)    0           conv1d[0][0]                     \n",
      "                                                                 tf.compat.v1.nn.softmax[0][0]    \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.getitem (Slici (8, 1999, 512)       0           multiply[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "tf.__operators__.getitem_1 (Sli (8, 1999, 512)       0           multiply[0][0]                   \n",
      "__________________________________________________________________________________________________\n",
      "Decoder_Num0 (Conv1DTranspose)  (8, 16000, 1)        8192        tf.__operators__.getitem[0][0]   \n",
      "__________________________________________________________________________________________________\n",
      "Decoder_Num1 (Conv1DTranspose)  (8, 16000, 1)        8192        tf.__operators__.getitem_1[0][0] \n",
      "__________________________________________________________________________________________________\n",
      "tf.concat (TFOpLambda)          (8, 16000, 2)        0           Decoder_Num0[0][0]               \n",
      "                                                                 Decoder_Num1[0][0]               \n",
      "==================================================================================================\n",
      "Total params: 5,051,520\n",
      "Trainable params: 5,051,520\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n",
      "Total parameters: 5051520\n"
     ]
    }
   ],
   "source": [
    "import import_ipynb\n",
    "from CV_Tasnet_Model import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_base_dir = \"C:/Users/Phil/anaconda3/envs/LibriMix/storage_dir/Libri2Mix/wav8k/min\"\n",
    "\n",
    "train_input_path = 'train-360/mix_both'\n",
    "train_target_paths = ['train-360/s1', 'train-360/s2']\n",
    "\n",
    "val_input_path = 'dev/mix_both'\n",
    "val_target_paths = ['dev/s1', 'dev/s2']\n",
    "\n",
    "test_input_path = 'test/mix_both'\n",
    "test_target_paths = ['test/s1', 'test/s2']\n",
    "\n",
    "ext = '/*.wav'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store a path\n",
    "train_input_dir = os.path.join(data_base_dir, train_input_path)\n",
    "train_target_dirs = [os.path.join(data_base_dir, train_path) \n",
    "                     for train_path in train_target_paths]\n",
    "\n",
    "val_input_dir = os.path.join(data_base_dir, val_input_path)\n",
    "val_target_dirs =[os.path.join(data_base_dir, val_path) \n",
    "                  for val_path in val_target_paths]\n",
    "\n",
    "test_input_dir = os.path.join(data_base_dir, test_input_path)\n",
    "test_target_dirs = [os.path.join(data_base_dir, test_path)\n",
    "                   for test_path in test_target_paths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find every files in a selected path\n",
    "train_input_files = glob.glob(train_input_dir + ext)\n",
    "train_target_files = [glob.glob(train_path + ext) for train_path in train_target_dirs]\n",
    "\n",
    "val_input_files = glob.glob(val_input_dir + ext)\n",
    "val_target_files = [glob.glob(val_path + ext) for val_path in val_target_dirs]\n",
    "\n",
    "test_input_files = glob.glob(test_input_dir + ext)\n",
    "test_target_files = [glob.glob(test_path + ext) for test_path in test_target_dirs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of training files: 50800\n",
      "Number of validation files: 3000\n",
      "Number of test files: 3000\n"
     ]
    }
   ],
   "source": [
    "print('Number of training files:', len(train_input_files))\n",
    "for i in [len(j) for j in train_target_files]:\n",
    "    assert len(train_input_files) == i\n",
    "\n",
    "print('Number of validation files:', len(val_input_files))\n",
    "for i in [len(j) for j in val_target_files]:\n",
    "    assert len(val_input_files) == i\n",
    "\n",
    "print('Number of test files:', len(test_input_files))\n",
    "for i in [len(j) for j in test_target_files]:\n",
    "    assert len(test_input_files) == i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import multi_sf_blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "import logging\n",
    "logging.getLogger('tensorflow').setLevel(logging.ERROR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:/Users/Phil/anaconda3/envs/CTnet\\\\checkpoints/cp-{epoch:03d}.ckpt'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_base_dir = \"C:/Users/Phil/anaconda3/envs/CTnet\"\n",
    "checkpoint_path = \"checkpoints/cp-{epoch:03d}.ckpt\"\n",
    "saving_path = os.path.join(model_base_dir, checkpoint_path)\n",
    "saving_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_callback = keras.callbacks.ModelCheckpoint(filepath=saving_path,\n",
    "                                                 verbose=1,\n",
    "                                                 save_weights_only=True,\n",
    "                                                 save_freq='epoch')\n",
    "callbacks = keras.callbacks.CallbackList([model_callback], model=cv_tasnet)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:/Users/Phil/anaconda3/envs/CTnet\\\\logs'"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_path = os.path.join(model_base_dir, \"logs\")\n",
    "summary_writer = tf.summary.create_file_writer(log_path)\n",
    "log_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "## tf.function 쓰면 안 됨\n",
    "def train_step(input_batch, target1_batch, target2_batch):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predicted_batch = cv_tasnet(input_batch, training=True)\n",
    "        loss = calculate_sisnr(predicted_batch, tf.cast(tf.concat([target1_batch, target2_batch], axis=-1), dtype=tf.float32),\n",
    "                              BATCH_SIZE=BATCH_SIZE, num_spks=num_spks)\n",
    "    trainable_vars = cv_tasnet.trainable_variables\n",
    "    gradients = tape.gradient(loss, trainable_vars)\n",
    "    cv_tasnet.optimizer.apply_gradients(zip(gradients, trainable_vars))\n",
    "    \n",
    "    return {'train_sisnr': loss}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_step(input_batch, target1_batch, target2_batch):\n",
    "    predicted_batch = cv_tasnet(input_batch, training=False)\n",
    "    loss = calculate_sisnr(predicted_batch, tf.cast(tf.concat([target1_batch, target2_batch], axis=-1), dtype=tf.float32),\n",
    "                          BATCH_SIZE=BATCH_SIZE, num_spks=num_spks)\n",
    "    return {'val_sisnr': loss}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "################### Start of Epoch Num 1, out of 3 Epochs ###################\n",
      "========================== Training part Begins ==========================\n",
      "Number of 2-seconds long samples: 46\n",
      "Expected iterations for train: 4\n",
      "...\n",
      "Number of 2-seconds long samples: 46\n",
      "Expected iterations for train: 4\n",
      "...\n",
      "Number of 2-seconds long samples: 53\n",
      "Expected iterations for train: 5\n",
      "...\n",
      "==== Training part Done, for 2 iterations, time taken: 241.58 seconds ====\n",
      "\n",
      "========================== Validation part Begins ==========================\n",
      "Number of 2-seconds long samples: 23\n",
      "Expected iterations for validation: 1\n",
      ",,\n",
      "Number of 2-seconds long samples: 22\n",
      "Expected iterations for validation: 1\n",
      ",,\n",
      "Number of 2-seconds long samples: 18\n",
      "Expected iterations for validation: 1\n",
      ",,\n",
      "==== Validation part Done, for 1 iterations, time taken: 57.74 seconds ====\n",
      "\n",
      "Epoch 00001: saving model to C:/Users/Phil/anaconda3/envs/CTnet\\checkpoints\\cp-001.ckpt\n",
      "############### End of Epoch Num 1, time taken: 311.95 seconds ###############\n",
      "\n",
      "################### Start of Epoch Num 2, out of 3 Epochs ###################\n",
      "========================== Training part Begins ==========================\n",
      "Number of 2-seconds long samples: 47\n",
      "Expected iterations for train: 4\n",
      "...\n",
      "Number of 2-seconds long samples: 34\n",
      "Expected iterations for train: 3\n",
      "...\n",
      "Number of 2-seconds long samples: 46\n",
      "Expected iterations for train: 4\n",
      "...\n",
      "==== Training part Done, for 2 iterations, time taken: 232.59 seconds ====\n",
      "\n",
      "========================== Validation part Begins ==========================\n",
      "Number of 2-seconds long samples: 18\n",
      "Expected iterations for validation: 1\n",
      ",,\n",
      "Number of 2-seconds long samples: 28\n",
      "Expected iterations for validation: 2\n",
      ",,\n",
      "Number of 2-seconds long samples: 23\n",
      "Expected iterations for validation: 1\n",
      ",,\n",
      "==== Validation part Done, for 1 iterations, time taken: 57.70 seconds ====\n",
      "\n",
      "Epoch 00002: saving model to C:/Users/Phil/anaconda3/envs/CTnet\\checkpoints\\cp-002.ckpt\n",
      "############### End of Epoch Num 2, time taken: 302.31 seconds ###############\n",
      "\n"
     ]
    }
   ],
   "source": [
    "EPOCHS = 3\n",
    "per_step = 1500 # 램과 상관이 있음\n",
    "N_steps = len(train_input_files) // per_step\n",
    "val_N_steps = len(val_input_files) // per_step\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    train_metrics = []\n",
    "    val_metrics = []\n",
    "    for epoch in range(EPOCHS):\n",
    "        print('################### Start of Epoch Num {}, out of {} Epochs ###################'.format(epoch+1, \n",
    "                                                                                               EPOCHS))\n",
    "        epoch_time = time.time()\n",
    "        idx_lst = [i for i in range(len(train_input_files))]\n",
    "        random.shuffle(idx_lst)\n",
    "\n",
    "        train_time = time.time()\n",
    "        print('========================== Training part Begins ==========================')\n",
    "        for train_step in range(N_steps):\n",
    "            drawn_numbers, idx_lst = random_int(idx_lst, per_step)\n",
    "            selected_input_files, selected_target_files1, selected_target_files2 = get_file_lst(drawn_numbers,\n",
    "                                        train_input_files, train_target_files[0], train_target_files[1])\n",
    "            pool = multiprocessing.Pool(processes=8)\n",
    "            segments = pool.map(multi_sf_blocks.get_sf_blocks, [selected_input_files,\n",
    "                                                               selected_target_files1,\n",
    "                                                               selected_target_files2])\n",
    "#             print('Number of 2-seconds long samples:', len(segments[0]))\n",
    "            print('Expected iterations for train:', len(segments[0])//BATCH_SIZE - 1, end='\\t')\n",
    "            train_ds = generate_ds(segments[0], segments[1], segments[2])         \n",
    "            \n",
    "            train_sisnr = []\n",
    "            for i, (input_batch, target_batch1, target_batch2) in enumerate(train_ds):\n",
    "                loss = train_step(input_batch, target_batch1, target_batch2)\n",
    "                train_sisnr.append(loss['train_sisnr'].numpy())\n",
    "            print('.', end='')\n",
    "            train_metrics.append(tf.reduce_mean(train_sisnr))\n",
    "            with summary_writer.as_default():\n",
    "                tf.summary.scalar('train_sisnr', tf.reduce_mean(train_sisnr), step=step*(epoch+1))\n",
    "            print()\n",
    "        print('==== Training part Done, for {} iterations, time taken: {:.2f} seconds ===='.format(i,\n",
    "                                                                                time.time()-train_time))\n",
    "        print()\n",
    "        \n",
    "        \n",
    "        ## validation part\n",
    "        val_idx_lst = [i for i in range(len(val_input_files))]\n",
    "        random.shuffle(val_idx_lst)\n",
    "\n",
    "        val_time = time.time()\n",
    "        print('========================== Validation part Begins ==========================')\n",
    "        for val_step in range(val_N_steps):\n",
    "            drawn_numbers, val_idx_lst = random_int(val_idx_lst, per_step)\n",
    "            val_selected_input_files, val_selected_target_files1, val_selected_target_files2 = get_file_lst(\n",
    "                drawn_numbers, val_input_files, val_target_files[0], val_target_files[1])\n",
    "            val_segments = pool.map(multi_sf_blocks.get_sf_blocks, [val_selected_input_files,\n",
    "                                                                   val_selected_target_files1,\n",
    "                                                                   val_selected_target_files2])\n",
    "#             print('Number of 2-seconds long samples:', len(val_segments[0]))\n",
    "            print('Expected iterations for validation:', len(val_segments[0])//BATCH_SIZE - 1)\n",
    "            val_ds = generate_ds(val_segments[0], val_segments[1], val_segments[2])\n",
    "\n",
    "            val_sisnr = []\n",
    "            for j, (val_input_batch, val_target_batch1, val_target_batch2) in enumerate(val_ds):\n",
    "                val_loss = test_step(val_input_batch, val_target_batch1, val_target_batch2)\n",
    "                val_sisnr.append(val_loss['val_sisnr'].numpy())\n",
    "            print(',', end='')\n",
    "                \n",
    "            val_metrics.append(tf.reduce_mean(val_sisnr))\n",
    "            with summary_writer.as_default():\n",
    "                tf.summary.scalar('val_sisnr', tf.reduce_mean(val_sisnr), step=val_step*(epoch+1))\n",
    "            print()\n",
    "        print('==== Validation part Done, for {} iterations, time taken: {:.2f} seconds ===='.format(j,\n",
    "                                                                                    time.time()-val_time))\n",
    "\n",
    "        logs = {'train_sisnr': tf.reduce_mean(train_metrics),\n",
    "           'val_sisnr': tf.reduce_mean(val_metrics)}\n",
    "        callbacks.on_epoch_end(epoch, logs)\n",
    "\n",
    "        print('############### End of Epoch Num {}, time taken: {:.2f} seconds ###############'.format(epoch + 1, \n",
    "                                                                                    time.time() - epoch_time))\n",
    "        print()\n",
    "        print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latest = tf.train.latest_checkpoint(\"C:/Users/Phil/anaconda3/envs/CTnet/checkpoints\")\n",
    "latest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cv_tasnet.set_weights()로는 안 됨\n",
    "cv_tasnet.load_weights(latest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
